api:
  class: ModelStatistics
  method: GetAverageInferenceTime
  signature: float ModelStatistics::GetAverageInferenceTime() const
documentation:
  brief: Calculates and returns the average inference time of a model in milliseconds.
  description: The GetAverageInferenceTime method computes the average time taken
    by a model to perform a single inference operation. This value is typically used
    for performance monitoring, optimization, and debugging purposes within the TrinityCore
    framework. The method aggregates timing data from previous inference runs and
    returns the arithmetic mean as a floating-point value representing milliseconds.
    It is commonly utilized in AI-driven systems or machine learning modules where
    model performance is critical.
  parameters: []
  returns: A float value representing the average inference time in milliseconds.
    Returns 0.0f if no inference data is available or if the model has not been initialized
    properly.
  examples:
  - title: Basic Usage
    code: 'ModelStatistics stats;

      float avgTime = stats.GetAverageInferenceTime();

      std::cout << "Average Inference Time: " << avgTime << " ms" << std::endl;'
    language: cpp
  - title: Performance Monitoring
    code: "ModelStatistics modelStats;\n// After running several inferences\nfloat\
      \ average = modelStats.GetAverageInferenceTime();\nif (average > 100.0f) {\n\
      \    // Log performance warning\n    sLog->outString(\"Warning: Model inference\
      \ time exceeded threshold.\");\n}"
    language: cpp
  notes: This method assumes that timing data has been collected through prior calls
    to model inference functions. The accuracy of the returned value depends on the
    number and consistency of previous measurements. If no inferences have occurred,
    or if the internal tracking mechanism fails, the method may return an inaccurate
    or zero value.
  warnings: Ensure that the ModelStatistics object is properly initialized before
    calling this method. Calling it on an uninitialized or corrupted instance may
    lead to undefined behavior. Additionally, this method does not account for external
    factors such as system load or hardware variability which could affect actual
    inference times.
  related:
  - GetTotalInferenceTime
  - GetInferenceCount
  - ResetStatistics
metadata:
  confidence: 0.9
  generated_at: '2025-11-08T16:18:37.104681'
  generator: lmstudio-qwen3-coder-30b
  version: 1.0.0
  source: core
