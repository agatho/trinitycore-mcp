api:
  class: MLPerformanceTracker
  method: SetTargetInferenceTime
  signature: void MLPerformanceTracker::SetTargetInferenceTime(uint64_t microseconds)
documentation:
  brief: Sets the target inference time for machine learning performance tracking
    in microseconds.
  description: The SetTargetInferenceTime method configures the expected execution
    time threshold for machine learning model inference operations within the TrinityCore
    framework. This value is used by the MLPerformanceTracker to monitor and evaluate
    the efficiency of AI-driven systems, particularly those involved in NPC behavior,
    pathfinding, or decision-making processes. The method accepts a uint64_t parameter
    representing the desired inference time in microseconds, which helps establish
    performance benchmarks for AI subsystems. This setting directly influences how
    the tracker evaluates whether ML operations are meeting expected performance criteria
    and can be used to trigger alerts or adjustments when actual execution times exceed
    the configured target.
  parameters:
  - name: microseconds
    description: The target inference time for ML operations, specified in microseconds.
      This value defines the maximum acceptable execution time for machine learning
      model inferences. Valid values are positive integers representing time durations;
      larger values indicate more lenient performance thresholds.
  returns: null
  examples:
  - title: Setting Target Inference Time
    code: 'MLPerformanceTracker tracker;

      tracker.SetTargetInferenceTime(5000); // Set target to 5 milliseconds'
    language: cpp
  - title: Adjusting Performance Thresholds for Different AI Systems
    code: 'MLPerformanceTracker npcBehaviorTracker;

      npcBehaviorTracker.SetTargetInferenceTime(10000); // 10ms for NPC decision making


      MLPerformanceTracker pathfindingTracker;

      pathfindingTracker.SetTargetInferenceTime(25000); // 25ms for path calculations'
    language: cpp
  notes: This method is typically used in conjunction with other ML performance monitoring
    features to maintain optimal AI system performance. The target inference time
    should be set based on empirical testing and the specific requirements of the
    AI subsystem being monitored. Changes to this value take effect immediately for
    subsequent performance measurements.
  warnings: Setting this value too low may cause false positive performance alerts
    even when AI systems are operating normally. Conversely, setting it too high may
    mask actual performance degradation issues. The value should be carefully tuned
    based on system capabilities and expected workload characteristics.
  related:
  - GetTargetInferenceTime
  - RecordInferenceTime
  - IsPerformanceWithinThreshold
metadata:
  confidence: 0.9
  generated_at: '2025-11-04T00:24:39.431851'
  generator: lmstudio-qwen3-coder-30b
  version: 1.0.0
  source: core
