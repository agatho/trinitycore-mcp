api:
  class: NeuralLayer
  method: Backward
  signature: void NeuralLayer::Backward(const int & error, float learningRate)
documentation:
  brief: Performs backpropagation on the neural layer using the provided error and
    learning rate values.
  description: The Backward method executes the backward pass of a neural network
    layer, updating weights and biases based on the computed error gradient. This
    method is typically called during training to adjust the layer's parameters in
    response to prediction errors. The error parameter represents the gradient of
    the loss function with respect to the layer's output, while the learning rate
    controls the step size of the weight updates. This implementation assumes the
    layer has already computed forward pass values and gradients are available for
    updating.
  parameters:
  - name: error
    description: The error gradient from the subsequent layer or loss function, representing
      how much the output contributed to the overall prediction error.
  - name: learningRate
    description: A scalar value that determines the step size of parameter updates
      during backpropagation. Must be positive and typically ranges between 0.001
      and 0.1 for stable training.
  returns: null
  examples:
  - title: Basic Backward Pass Usage
    code: 'NeuralLayer layer;

      int error = 5;

      float learningRate = 0.01f;

      layer.Backward(error, learningRate);'
    language: cpp
  - title: Training Loop Integration
    code: "for (int epoch = 0; epoch < maxEpochs; ++epoch) {\n    // Forward pass\n\
      \    layer.Forward(input);\n    \n    // Compute error\n    int error = computeLoss(target,\
      \ output);\n    \n    // Backward pass\n    layer.Backward(error, learningRate);\n\
      }"
    language: cpp
  notes: This method modifies internal weights and biases of the neural layer in-place.
    It is expected that the forward pass has been completed before calling this method.
    The error parameter should be properly computed from the loss function or previous
    layer's gradients.
  warnings: Calling Backward without a preceding forward pass will result in undefined
    behavior. Ensure that all necessary intermediate values are stored during the
    forward pass for correct gradient computation. Using excessively high learning
    rates may cause training instability or divergence.
  related:
  - NeuralLayer::Forward
  - NeuralLayer::UpdateWeights
  - NeuralLayer::GetWeights
metadata:
  confidence: 0.85
  generated_at: '2025-11-08T23:14:17.934492'
  generator: lmstudio-qwen3-coder-30b
  version: 1.0.0
  source: core
