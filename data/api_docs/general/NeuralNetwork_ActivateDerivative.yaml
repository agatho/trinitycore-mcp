api:
  class: NeuralNetwork
  method: ActivateDerivative
  signature: float NeuralNetwork::ActivateDerivative(float x, ActivationFunction func)
    const
documentation:
  brief: Computes the derivative of the specified activation function at the given
    input value.
  description: 'The ActivateDerivative method calculates the derivative of an activation
    function used in neural network computations. This is a core mathematical operation
    required for backpropagation during training. The method accepts an input value
    ''x'' and an activation function type, then returns the computed derivative. In
    the context of TrinityCore''s AI systems, this method supports neural network-based
    decision making and learning algorithms. The implementation assumes that the activation
    function has a well-defined derivative at the given point. Precondition: The input
    ''x'' should be within the domain of the specified activation function. Postcondition:
    Returns a float representing the derivative value.'
  parameters:
  - name: x
    description: The input value at which to compute the derivative of the activation
      function
  - name: func
    description: The activation function type for which to compute the derivative.
      Must be a valid ActivationFunction enum value.
  returns: A float representing the derivative of the specified activation function
    at input 'x'. Returns 0.0 if an invalid activation function is provided.
  examples:
  - title: Basic Usage
    code: 'NeuralNetwork nn;

      float result = nn.ActivateDerivative(0.5f, ActivationFunction::SIGMOID);

      // Computes derivative of sigmoid at x=0.5'
    language: cpp
  - title: Training Context
    code: 'NeuralNetwork nn;

      float input = 2.0f;

      float gradient = nn.ActivateDerivative(input, ActivationFunction::TANH);

      // Used in backpropagation for computing gradients'
    language: cpp
  notes: This method is typically used internally during neural network training phases.
    The derivative computation is essential for gradient descent algorithms. Performance
    is optimized for frequent calls during training cycles. The method does not perform
    bounds checking on the input value 'x' - it assumes valid inputs.
  warnings: Ensure that the ActivationFunction enum values are properly defined and
    supported by the implementation. Using an undefined or unsupported activation
    function may return unexpected results. This method should only be called with
    valid neural network contexts where derivatives are required for training.
  related:
  - Activate
  - Train
  - ForwardPass
metadata:
  confidence: 0.9
  generated_at: '2025-11-07T23:04:13.246385'
  generator: lmstudio-qwen3-coder-30b
  version: 1.0.0
  source: core
