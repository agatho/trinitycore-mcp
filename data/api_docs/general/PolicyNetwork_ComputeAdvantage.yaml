api:
  class: PolicyNetwork
  method: ComputeAdvantage
  signature: void PolicyNetwork::ComputeAdvantage(int & trajectory, float gamma)
documentation:
  brief: Computes the advantage value for a given trajectory using gamma discounting.
  description: The ComputeAdvantage method calculates the advantage of a specific
    trajectory within the policy network by applying temporal difference discounting
    with the provided gamma parameter. This is commonly used in reinforcement learning
    contexts to evaluate how much better a particular action sequence is compared
    to the average outcome. The method modifies the input trajectory reference, likely
    updating it with computed advantage values or indices. It operates on the internal
    state of the PolicyNetwork and assumes that prior computations or data preparation
    has been completed.
  parameters:
  - name: trajectory
    description: Reference to an integer representing the trajectory index or identifier
      to compute advantage for. The value may be modified by this method.
  - name: gamma
    description: Discount factor used in temporal difference calculations, typically
      a float between 0 and 1. Lower values prioritize immediate rewards, while higher
      values consider long-term outcomes more heavily.
  returns: null
  examples:
  - title: Basic Advantage Computation
    code: 'int traj = 5;

      float gamma = 0.9f;

      PolicyNetwork network;

      network.ComputeAdvantage(traj, gamma);

      // traj now contains updated advantage value or index'
    language: cpp
  notes: This method likely operates within a reinforcement learning framework where
    trajectories represent sequences of actions and rewards. The internal implementation
    may involve recursive or iterative calculations based on reward propagation through
    time steps.
  warnings: null
  related: []
metadata:
  confidence: 0.85
  generated_at: '2025-11-06T05:46:28.565377'
  generator: lmstudio-qwen3-coder-30b
  version: 1.0.0
  source: core
